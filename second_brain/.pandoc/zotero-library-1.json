[
  {"id":"abdullaHowWuduStep","accessed":{"date-parts":[["2025",10,28]]},"author":[{"family":"Abdulla","given":"Ahmed"}],"citation-key":"abdullaHowWuduStep","container-title":"My Islam","title":"How To Do Wudu - Step By Step For Beginners (2024 Guide)","type":"webpage","URL":"https://myislam.org/how-to-do-wudu/"},
  {"id":"alammarIllustratedBERTELMo","abstract":"Discussions:\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\n\n\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\n\n2021 Update: I created this brief and highly accessible video intro to BERT\n\n\n\n\n\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).","accessed":{"date-parts":[["2025",10,26]]},"author":[{"family":"Alammar","given":"Jay"}],"citation-key":"alammarIllustratedBERTELMo","container-title":"The Illustrated BERT","genre":"Blog","language":"eng","title":"The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)","type":"webpage","URL":"https://jalammar.github.io/illustrated-bert/"},
  {"id":"Chapter2How","accessed":{"date-parts":[["2025",10,28]]},"citation-key":"Chapter2How","container-title":"Masjid ar-Rahmah | Mosque of Mercy","language":"eng","title":"Chapter 2 - How to Make Wudu, Step by Step - Masjid ar-Rahmah | Mosque of Mercy","title-short":"Chapter 2 - How to Make Wudu, Step by Step","type":"webpage","URL":"https://www.mymasjid.ca/beginners-guide-learn-pray-salah/chapter-2/"},
  {"id":"EgyptsDarAlIfta","accessed":{"date-parts":[["2025",10,28]]},"citation-key":"EgyptsDarAlIfta","container-title":"Egypt's Dar Al-Ifta","language":"eng","title":"Egypt's Dar Al-Ifta | How to perform Wudhu (Ablution)","title-short":"How to perform Wudhu (Ablution)","type":"webpage","URL":"https://www.dar-alifta.org/en/article/details/249/how-to-perform-wudhu-ablution"},
  {"id":"HowMakeWudu","accessed":{"date-parts":[["2025",10,28]]},"citation-key":"HowMakeWudu","container-title":"Islamc Question & Answer","genre":"Forum","language":"eng","title":"How to Make Wudu - Islam Question & Answer","title-short":"How to Make Wudu","type":"webpage","URL":"https://islamqa.info/en/answers/11497/how-to-make-wudu"},
  {"id":"meemPATQuestionsSelfUpdatingBenchmark2024","abstract":"Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g. ‘Who was the US president in 1970?’). Little work has studied questions whose temporal context is relative to the present time (e.g. ‘Who was the previous US president?’). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. ‘before’, ‘previous’) are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model (TEMPREASON-T5) on PAT-Questions through direct prompting and retrieval-augmented generation (RAG). The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities.","accessed":{"date-parts":[["2025",10,26]]},"author":[{"family":"Meem","given":"Jannat Ara"},{"family":"Rashid","given":"Muhammad Shihab"},{"family":"Dong","given":"Yue"},{"family":"Hristidis","given":"Vagelis"}],"citation-key":"meemPATQuestionsSelfUpdatingBenchmark2024","DOI":"10.48550/arXiv.2402.11034","issued":{"date-parts":[["2024",6,3]]},"language":"en","number":"arXiv:2402.11034","publisher":"arXiv","source":"arXiv.org","title":"PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering","title-short":"PAT-Questions","type":"article","URL":"http://arxiv.org/abs/2402.11034"},
  {"id":"raschkaBuildLargeLanguage2025","abstract":"Build a Large Language Model (From Scratch) is a practical and eminently-satisfying hands-on journey into the foundations of generative AI. Without relying on any existing LLM libraries, you'll code a base model, evolve it into a text classifier, and ultimately create a chatbot that can follow your conversational instructions. And you'll really understand it because you built it yourself!","author":[{"family":"Raschka","given":"Sebastian"}],"citation-key":"raschkaBuildLargeLanguage2025","collection-title":"From scratch series","event-place":"Shelter Island","ISBN":"978-1-63835-573-1","issued":{"date-parts":[["2025"]]},"language":"eng","number-of-pages":"1","publisher":"Manning","publisher-place":"Shelter Island","source":"K10plus ISBN","title":"Build a Large Language Model (from scratch)","type":"book"},
  {"id":"rashidEcoRankBudgetConstrainedText2024","abstract":"Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs. Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware supervised and unsupervised baselines.","accessed":{"date-parts":[["2025",10,26]]},"author":[{"family":"Rashid","given":"Muhammad Shihab"},{"family":"Meem","given":"Jannat Ara"},{"family":"Dong","given":"Yue"},{"family":"Hristidis","given":"Vagelis"}],"citation-key":"rashidEcoRankBudgetConstrainedText2024","DOI":"10.48550/arXiv.2402.10866","issued":{"date-parts":[["2024",5,28]]},"language":"en","number":"arXiv:2402.10866","publisher":"arXiv","source":"arXiv.org","title":"EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models","title-short":"EcoRank","type":"article","URL":"http://arxiv.org/abs/2402.10866"},
  {"id":"rashidNORMYNonUniformHistory2024","abstract":"Open Retrieval Conversational Question Answering (OrConvQA) answers a question given a conversation as context and a document collection. A typical OrConvQA pipeline consists of three modules: a Retriever to retrieve relevant documents from the collection, a Reranker to rerank them given the question and the context, and a Reader to extract an answer span. The conversational turns can provide valuable context to answer the final query. State-of-the-art OrConvQA systems use the same history modeling for all three modules of the pipeline. We hypothesize this as suboptimal. Specifically, we argue that a broader context is needed in the first modules of the pipeline to not miss relevant documents, while a narrower context is needed in the last modules to identify the exact answer span. We propose NORMY, the first unsupervised non-uniform history modeling pipeline which generates the best conversational history for each module. We further propose a novel Retriever for NORMY, which employs keyphrase extraction on the conversation history, and leverages passages retrieved in previous turns as additional context. We also created a new dataset for OrConvQA, by expanding the doc2dial dataset. We implemented various state-of-the-art history modeling techniques and comprehensively evaluated them separately for each module of the pipeline on three datasets: OR-QUAC, our doc2dial extension, and ConvMix. Our extensive experiments show that NORMY outperforms the state-of-the-art in the individual modules and in the end-to-end system.","accessed":{"date-parts":[["2025",10,26]]},"author":[{"family":"Rashid","given":"Muhammad Shihab"},{"family":"Meem","given":"Jannat Ara"},{"family":"Hristidis","given":"Vagelis"}],"citation-key":"rashidNORMYNonUniformHistory2024","DOI":"10.48550/arXiv.2402.04548","issued":{"date-parts":[["2024",2,7]]},"language":"en","number":"arXiv:2402.04548","publisher":"arXiv","source":"arXiv.org","title":"NORMY: Non-Uniform History Modeling for Open Retrieval Conversational Question Answering","title-short":"NORMY","type":"article","URL":"http://arxiv.org/abs/2402.04548"},
  {"id":"rushAnnotatedTransformer","accessed":{"date-parts":[["2025",10,26]]},"author":[{"family":"Rush","given":"Sasha"}],"citation-key":"rushAnnotatedTransformer","container-title":"The Annotated Transformer","language":"eng","title":"The Annotated Transformer","type":"webpage","URL":"https://nlp.seas.harvard.edu/annotated-transformer/"},
  {"id":"vaswaniAttentionAllYou2023","abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","accessed":{"date-parts":[["2025",10,26]]},"author":[{"family":"Vaswani","given":"Ashish"},{"family":"Shazeer","given":"Noam"},{"family":"Parmar","given":"Niki"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Jones","given":"Llion"},{"family":"Gomez","given":"Aidan N."},{"family":"Kaiser","given":"Lukasz"},{"family":"Polosukhin","given":"Illia"}],"citation-key":"vaswaniAttentionAllYou2023","DOI":"10.48550/arXiv.1706.03762","issued":{"date-parts":[["2023",8,2]]},"language":"en","number":"arXiv:1706.03762","publisher":"arXiv","source":"arXiv.org","title":"Attention Is All You Need","type":"article","URL":"http://arxiv.org/abs/1706.03762"},
  {"id":"wahidHowPerformWudu2025","accessed":{"date-parts":[["2025",10,28]]},"author":[{"family":"Wahid","given":"Mufti Samir"}],"citation-key":"wahidHowPerformWudu2025","container-title":"WikiHow","issued":{"date-parts":[["2025",8,7]]},"language":"eng","title":"How to Perform Wudu: 12 Steps (with Pictures) - wikiHow","title-short":"How to Perform Wudu: 12 Steps","type":"webpage","URL":"https://www.wikihow.com/Perform-Wudu"},
  {"id":"zotero-item-5","citation-key":"zotero-item-5","type":"book"}
]
